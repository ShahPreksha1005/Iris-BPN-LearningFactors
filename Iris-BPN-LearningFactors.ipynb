{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0hYky-GhM2O"
   },
   "source": [
    "#**Iris-BPN-LearningFactors**\n",
    "\n",
    "###**Created by Preksha Shah**\n",
    "\n",
    "##**Introduction**\n",
    "**Backpropagation Neural Network (BPN)** is a crucial algorithm in training artificial neural networks, especially for tasks requiring accurate predictions. It consists of two phases: forward propagation and backward propagation.\n",
    "\n",
    "1. **Forward Propagation:** The input data is passed through the network layer by layer. Each neuron processes the input using weights and an activation function, producing an output. The final output is compared to the actual target value, and the error is calculated.\n",
    "\n",
    "2. **Backward Propagation:** The error is propagated back through the network to update the weights. This involves calculating the gradient of the loss function with respect to each weight using the chain rule. The weights are then adjusted to minimize the error, guided by the learning rate and potentially enhanced by momentum.\n",
    "\n",
    "The **learning factors** like learning rate and momentum play a significant role in how quickly and effectively the network learns. A well-tuned BPN adjusts these factors to optimize learning, resulting in faster convergence and better performance on tasks like classification or regression.\n",
    "\n",
    "---\n",
    "\n",
    "#**Q4: Create a program to implement learning factors in Back Propagation Neural Networks focusing on steps such as data handling, network architecture and implement learning factors for better learning. In the manual way done above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG71sHWEhUDD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgyoSrj7iY4R"
   },
   "outputs": [],
   "source": [
    "# Activation function (Sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpoF2yDWkNFa"
   },
   "outputs": [],
   "source": [
    "# Derivative of sigmoid (for backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mb1kibC4lJs0"
   },
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fSNnufMlQUY"
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XybTSnulQo9",
    "outputId": "63a1812f-2aae-4271-993a-582531788742"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding of the target variable\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwN8GZKElQzi"
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c2bzRB1lQ8m"
   },
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "input_size = X_train.shape[1]  # Number of input features\n",
    "hidden_size = 8                # Number of hidden neurons\n",
    "output_size = y_train.shape[1] # Number of output neurons (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzZc0qpokPp4"
   },
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "np.random.seed(42)  # For reproducibility\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "bias_hidden = np.zeros((1, hidden_size))\n",
    "bias_output = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3DtH_T9kZBY"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoJZL80CljI3",
    "outputId": "fb374986-b168-4696-acca-e9fe24890b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris - Epoch 0, Loss: 0.51914\n",
      "Iris - Epoch 1000, Loss: 0.01007\n",
      "Iris - Epoch 2000, Loss: 0.00974\n",
      "Iris - Epoch 3000, Loss: 0.00666\n",
      "Iris - Epoch 4000, Loss: 0.00162\n",
      "Iris - Epoch 5000, Loss: 0.00070\n",
      "Iris - Epoch 6000, Loss: 0.00042\n",
      "Iris - Epoch 7000, Loss: 0.00029\n",
      "Iris - Epoch 8000, Loss: 0.00022\n",
      "Iris - Epoch 9000, Loss: 0.00017\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    final_output = sigmoid(final_input)\n",
    "\n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = np.mean(np.square(y_train - final_output))\n",
    "\n",
    "    # Backward pass\n",
    "    output_error = y_train - final_output\n",
    "    output_delta = output_error * sigmoid_derivative(final_output)\n",
    "\n",
    "    hidden_error = np.dot(output_delta, weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden_output += np.dot(hidden_output.T, output_delta) * learning_rate\n",
    "    bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "    weights_input_hidden += np.dot(X_train.T, hidden_delta) * learning_rate\n",
    "    bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Iris - Epoch {epoch}, Loss: {loss:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiZowB9SljLW"
   },
   "outputs": [],
   "source": [
    "# Test the network\n",
    "hidden_input_test = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
    "hidden_output_test = sigmoid(hidden_input_test)\n",
    "\n",
    "final_input_test = np.dot(hidden_output_test, weights_hidden_output) + bias_output\n",
    "final_output_test = sigmoid(final_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MF7QpZWxlnZY"
   },
   "outputs": [],
   "source": [
    "# Convert predictions to class labels\n",
    "predictions = np.argmax(final_output_test, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLaJBNrdlnbs",
    "outputId": "eadc88e2-2b3d-43af-98ba-f46a925d1659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iris - Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "print(f'\\nIris - Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0wFoSkklneF",
    "outputId": "92f3e098-458b-4efd-9f8e-444349f09f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iris - Final weights and biases:\n",
      "Weights (Input to Hidden):\n",
      "[[ 0.74406738  8.14909247  2.92758675 -1.18352539 -0.83794368 -0.29740104\n",
      "   2.06879149 -4.69173863]\n",
      " [-1.820428    9.36901765 -0.36812533 -2.63542388 -0.69381085  1.67104589\n",
      "  -1.28991436 -6.92123872]\n",
      " [ 2.75027356  3.24251504  1.71411134  0.11968824  4.42613326 -2.35605332\n",
      "   8.74383425  6.66158811]\n",
      " [ 2.78620161  2.61291191 -0.10983211 10.32763658  1.20048966 -2.59881698\n",
      "  -2.90342739  3.07353559]]\n",
      "Biases (Hidden Layer):\n",
      "[[ 3.03518785 -0.41913229  2.69027913 -8.98704902 -4.21893616 -1.96009524\n",
      "  -8.18181835 -4.96011173]]\n",
      "Weights (Hidden to Output):\n",
      "[[-4.58042906  9.19036429 -2.77315068]\n",
      " [-0.22030248 -7.88945175  7.97939643]\n",
      " [-2.26754515  5.59466627 -4.59267649]\n",
      " [-0.24849726 -9.71413406  9.43806367]\n",
      " [-1.88258642 -3.72509206  4.35907928]\n",
      " [ 5.085044   -5.85695234 -3.81977557]\n",
      " [-0.62344708 -9.32625261  8.89423374]\n",
      " [-0.92539162 -6.61466755  6.73311657]]\n",
      "Biases (Output Layer):\n",
      "[[ 1.17949232  0.31221633 -7.97040594]]\n",
      "\n",
      "Iris - Predictions after training:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Print final results\n",
    "print(\"\\nIris - Final weights and biases:\")\n",
    "print(\"Weights (Input to Hidden):\")\n",
    "print(weights_input_hidden)\n",
    "print(\"Biases (Hidden Layer):\")\n",
    "print(bias_hidden)\n",
    "print(\"Weights (Hidden to Output):\")\n",
    "print(weights_hidden_output)\n",
    "print(\"Biases (Output Layer):\")\n",
    "print(bias_output)\n",
    "\n",
    "print(\"\\nIris - Predictions after training:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lXDYoSVmEQP"
   },
   "source": [
    "### **Summary**\n",
    "Weights and Biases: These matrices are crucial for the network's function. They are adjusted during training to minimize the prediction error.\n",
    "Predictions: The output shows how well the network has learned to classify the test data based on the patterns it has learned from the training data.\n",
    "\n",
    "---\n",
    "\n",
    "##**Key Achievements**\n",
    "1. **Backpropagation Neural Network Implementation:**Successfully built a BPN using NumPy, covering both forward and backward propagation steps.\n",
    "Incorporated learning factors like the learning rate to improve training efficiency.\n",
    "2. **Data Handling:** Utilized the Iris dataset, applying feature standardization and one-hot encoding to prepare data for the network.\n",
    "3. **Network Architecture:** Designed a network with one hidden layer, initialized weights and biases, and set up the architecture for training.\n",
    "4. **Training and Evaluation:** Trained the network over 10,000 epochs, reducing loss significantly and achieving high accuracy on test data.\n",
    "Displayed key results, including final weights, biases, and predictions.\n",
    "\n",
    "##**Conclusion**\n",
    "The assignment effectively demonstrates the principles of Backpropagation Neural Networks (BPN), showcasing the implementation of forward and backward propagation, and the impact of learning factors. The successful training and evaluation on the Iris dataset confirm the network's ability to learn and make accurate predictions. The detailed results validate the network's performance and the effectiveness of the learning algorithm.\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
