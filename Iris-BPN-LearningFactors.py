# -*- coding: utf-8 -*-
"""Iris-BPN-LearningFactors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmHLKlzNkcfFQ5MMx-AhJK1XOGzNNac1

#**Iris BPN Learning Factors**    

###**Created by Preksha Shah**

##**Introduction**
**Backpropagation Neural Network (BPN)** is a crucial algorithm in training artificial neural networks, especially for tasks requiring accurate predictions. It consists of two phases: forward propagation and backward propagation.

1. **Forward Propagation:** The input data is passed through the network layer by layer. Each neuron processes the input using weights and an activation function, producing an output. The final output is compared to the actual target value, and the error is calculated.

2. **Backward Propagation:** The error is propagated back through the network to update the weights. This involves calculating the gradient of the loss function with respect to each weight using the chain rule. The weights are then adjusted to minimize the error, guided by the learning rate and potentially enhanced by momentum.

The **learning factors** like learning rate and momentum play a significant role in how quickly and effectively the network learns. A well-tuned BPN adjusts these factors to optimize learning, resulting in faster convergence and better performance on tasks like classification or regression.

---

#**Q4: Create a program to implement learning factors in Back Propagation Neural Networks focusing on steps such as data handling, network architecture and implement learning factors for better learning. In the manual way done above.**
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

# Activation function (Sigmoid)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of sigmoid (for backpropagation)
def sigmoid_derivative(x):
    return x * (1 - x)

# Load Iris dataset
iris = load_iris()
X = iris.data  # Feature data
y = iris.target  # Target labels

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One Hot Encoding of the target variable
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y.reshape(-1, 1))

# Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize network parameters
input_size = X_train.shape[1]  # Number of input features
hidden_size = 8                # Number of hidden neurons
output_size = y_train.shape[1] # Number of output neurons (classes)

# Initialize weights and biases
np.random.seed(42)  # For reproducibility
weights_input_hidden = np.random.rand(input_size, hidden_size)
weights_hidden_output = np.random.rand(hidden_size, output_size)
bias_hidden = np.zeros((1, hidden_size))
bias_output = np.zeros((1, output_size))

# Training parameters
epochs = 10000
learning_rate = 0.1

# Training the network
for epoch in range(epochs):
    # Forward pass
    hidden_input = np.dot(X_train, weights_input_hidden) + bias_hidden
    hidden_output = sigmoid(hidden_input)

    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output
    final_output = sigmoid(final_input)

    # Compute loss (Mean Squared Error)
    loss = np.mean(np.square(y_train - final_output))

    # Backward pass
    output_error = y_train - final_output
    output_delta = output_error * sigmoid_derivative(final_output)

    hidden_error = np.dot(output_delta, weights_hidden_output.T)
    hidden_delta = hidden_error * sigmoid_derivative(hidden_output)

    # Update weights and biases
    weights_hidden_output += np.dot(hidden_output.T, output_delta) * learning_rate
    bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += np.dot(X_train.T, hidden_delta) * learning_rate
    bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate

    # Print loss every 1000 epochs
    if epoch % 1000 == 0:
        print(f'Iris - Epoch {epoch}, Loss: {loss:.5f}')

# Test the network
hidden_input_test = np.dot(X_test, weights_input_hidden) + bias_hidden
hidden_output_test = sigmoid(hidden_input_test)

final_input_test = np.dot(hidden_output_test, weights_hidden_output) + bias_output
final_output_test = sigmoid(final_input_test)

# Convert predictions to class labels
predictions = np.argmax(final_output_test, axis=1)
true_labels = np.argmax(y_test, axis=1)

# Accuracy
accuracy = np.mean(predictions == true_labels)
print(f'\nIris - Test Accuracy: {accuracy:.2f}')

# Print final results
print("\nIris - Final weights and biases:")
print("Weights (Input to Hidden):")
print(weights_input_hidden)
print("Biases (Hidden Layer):")
print(bias_hidden)
print("Weights (Hidden to Output):")
print(weights_hidden_output)
print("Biases (Output Layer):")
print(bias_output)

print("\nIris - Predictions after training:")
print(predictions)

"""### **Summary**
Weights and Biases: These matrices are crucial for the network's function. They are adjusted during training to minimize the prediction error.
Predictions: The output shows how well the network has learned to classify the test data based on the patterns it has learned from the training data.

---

##**Key Achievements**
1. **Backpropagation Neural Network Implementation:**Successfully built a BPN using NumPy, covering both forward and backward propagation steps.
Incorporated learning factors like the learning rate to improve training efficiency.
2. **Data Handling:** Utilized the Iris dataset, applying feature standardization and one-hot encoding to prepare data for the network.
3. **Network Architecture:** Designed a network with one hidden layer, initialized weights and biases, and set up the architecture for training.
4. **Training and Evaluation:** Trained the network over 10,000 epochs, reducing loss significantly and achieving high accuracy on test data.
Displayed key results, including final weights, biases, and predictions.

##**Conclusion**
The assignment effectively demonstrates the principles of Backpropagation Neural Networks (BPN), showcasing the implementation of forward and backward propagation, and the impact of learning factors. The successful training and evaluation on the Iris dataset confirm the network's ability to learn and make accurate predictions. The detailed results validate the network's performance and the effectiveness of the learning algorithm.

---
---
"""